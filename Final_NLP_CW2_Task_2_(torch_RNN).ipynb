{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Dj8UUz7cHwUt8BdOQxT7uPRzme-WQe0-",
      "authorship_tag": "ABX9TyN6nHK3kskHl++EOlZHb0pA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "46b5745ec94d420898995a4f04d92be0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_65ababf4c3df4746934921b8dca6242a",
              "IPY_MODEL_bde8330d70344178a7113107e5dfa167",
              "IPY_MODEL_3a2df7bc36fd428d8a82367c7cde6b56"
            ],
            "layout": "IPY_MODEL_7656bcc08e6b4175bfc19a51ebb1ce49"
          }
        },
        "65ababf4c3df4746934921b8dca6242a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5e97d0fdcf6464ca30996af52a9f24b",
            "placeholder": "​",
            "style": "IPY_MODEL_5738c65c6fb640fd92a5d83ebdeca4e0",
            "value": "  0%"
          }
        },
        "bde8330d70344178a7113107e5dfa167": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4475a418e044433e8a5abc15070667d8",
            "max": 207,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19201ac17d194075820271e57b09186a",
            "value": 0
          }
        },
        "3a2df7bc36fd428d8a82367c7cde6b56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13e31c97ba54465f96d0d15924d0e951",
            "placeholder": "​",
            "style": "IPY_MODEL_a810405ccd7a4834b6339e26b1185bd2",
            "value": " 0/207 [00:00&lt;?, ?it/s]"
          }
        },
        "7656bcc08e6b4175bfc19a51ebb1ce49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5e97d0fdcf6464ca30996af52a9f24b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5738c65c6fb640fd92a5d83ebdeca4e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4475a418e044433e8a5abc15070667d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19201ac17d194075820271e57b09186a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13e31c97ba54465f96d0d15924d0e951": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a810405ccd7a4834b6339e26b1185bd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JonasVerbickas/GYMTABLER/blob/master/Final_NLP_CW2_Task_2_(torch_RNN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 460,
      "metadata": {
        "id": "NvI835jinwKk"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import collections\n",
        "import random\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from statistics import mean\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Preprocessing/Cleaning"
      ],
      "metadata": {
        "id": "osfpuTR7ozpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USE_STEMMING_INSTEAD_OF_LEMMATIZATION = False"
      ],
      "metadata": {
        "id": "g3A_06E5RQXz"
      },
      "execution_count": 461,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "if USE_STEMMING_INSTEAD_OF_LEMMATIZATION:\n",
        "  token_processing_fn = nltk.PorterStemmer().stem \n",
        "else:\n",
        "  token_processing_fn = nltk.stem.WordNetLemmatizer().lemmatize\n"
      ],
      "metadata": {
        "id": "x2oxf4e9pDRj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbb7ef3e-db1e-4703-fccf-bfc2ade7f6b5"
      },
      "execution_count": 462,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def specialIpodProcessing(path_to_dataset, all_other_reviews_as_string):\n",
        "  \"\"\"\n",
        "  Inserts [t] tags into text from 'ipod.txt'\n",
        "  At an average review length taken from all other reviews\n",
        "  \"\"\"\n",
        "  # calculate the average review length\n",
        "  review_lengths = []\n",
        "  curr_review_length = 0\n",
        "  for line in all_other_reviews_as_string.split('\\n'):\n",
        "    if line == '[t]':\n",
        "      if curr_review_length > 0:\n",
        "        review_lengths.append(curr_review_length)\n",
        "      curr_review_length = 0\n",
        "    else:\n",
        "      curr_review_length += 1\n",
        "  average_len = int(mean(review_lengths))\n",
        "  print(\"average_len used for ipod.txt =\", average_len)\n",
        "  # insert [t] every after each subset of `average_len` sentences\n",
        "  curr_review_length = 0\n",
        "  with open(os.path.join(path_to_dataset, 'ipod.txt')) as f:\n",
        "    for line in f.readlines():\n",
        "      if curr_review_length > average_len:\n",
        "        all_other_reviews_as_string += \"\\n[t]\"\n",
        "        curr_review_length = 0\n",
        "      all_other_reviews_as_string += line\n",
        "      curr_review_length += 1\n",
        "  return all_other_reviews_as_string"
      ],
      "metadata": {
        "id": "fdrQ_NYAAzid"
      },
      "execution_count": 463,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_concat_text_content_string(path_to_dataset):\n",
        "  \"\"\"\n",
        "  Iterate through all of txt files in a given directory\n",
        "  Concatenate the contents of each .txt file to a large string\n",
        "  (Special processing for ipod.txt is applied)\n",
        "  \"\"\"\n",
        "  file_contents = \"\"\n",
        "  for filename in os.listdir(path_to_dataset):\n",
        "    # skip readme\n",
        "    if filename in ['README.txt', 'ipod.txt']:\n",
        "      continue\n",
        "    # append contents of other files to file_contents string\n",
        "    starting_corpus_size = len(file_contents)\n",
        "    with open(os.path.join(path_to_dataset, filename)) as f:\n",
        "      file_contents += f.read()\n",
        "    print(\"After appending\", filename, \"corpus sized increased to\", starting_corpus_size, \"->\", len(file_contents))\n",
        "  ## special processing for ipod.txt\n",
        "  starting_corpus_size = len(file_contents)\n",
        "  file_contents = specialIpodProcessing(path_to_dataset, file_contents)\n",
        "  print(\"After appending ipod corpus sized increased to\", starting_corpus_size, \"->\", len(file_contents))\n",
        "  return file_contents"
      ],
      "metadata": {
        "id": "Clq3AOsulBaK"
      },
      "execution_count": 464,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def removeAnnotationSymbols(sentence):\n",
        "  \"\"\"\n",
        "  Remove all tags that aren't part of the original text\n",
        "  and should only be used for processing\n",
        "  \"\"\"\n",
        "  static_removals = ['[t]', '[u]','[p]','[s]','[cc]','[cs]']\n",
        "  for symbol in static_removals:\n",
        "    sentence = sentence.replace(symbol, '')\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "oiC-3gdUmucm"
      },
      "execution_count": 465,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculateSentement(sentiment_tags):\n",
        "  \"\"\"\n",
        "  Finds all positive/negative numbers that are within []\n",
        "  Returns their total sum\n",
        "  \"\"\"\n",
        "  semantic_tokens = re.findall('\\[([+-]\\d)\\]', sentiment_tags)\n",
        "  semantic_sum = 0\n",
        "  for token in semantic_tokens:\n",
        "    semantic_sum += int(token)\n",
        "  return semantic_sum"
      ],
      "metadata": {
        "id": "ttGNmX0onMtl"
      },
      "execution_count": 466,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_string_of_reviews_into_df(string_of_reviews):\n",
        "  \"\"\"\n",
        "  1. Apply casefolding\n",
        "  2. Expand contractions\n",
        "  3. Tokenize sentances\n",
        "  4. Ignore stop-words\n",
        "  5. Ignore tokens that don't contain any alphanumeric characters\n",
        "  6. Ignore sentiment analysis tokens e.g. `+3`\n",
        "  \"\"\"\n",
        "  list_of_reviews = string_of_reviews.split(\"[t]\\n\")\n",
        "  tokenized_sentence_sentiment_list = []\n",
        "  for review in list_of_reviews:\n",
        "    sentiment = 0\n",
        "    processed_review = []\n",
        "    review_split_into_lines = review.split('\\n')\n",
        "    for line in review_split_into_lines:\n",
        "      sentiment_tags, _, sentence = line.partition(\"##\")\n",
        "      sentiment += calculateSentement(sentiment_tags)\n",
        "      # I use `x` here instead of proper variable names\n",
        "      # because it allows me switch the order of statements without having to rename variables\n",
        "      # (This is very convinient when testing; and in my opinion makes the code more readable)\n",
        "      x = removeAnnotationSymbols(sentence)\n",
        "      x = x.casefold()\n",
        "      x = nltk.tokenize.word_tokenize(x)\n",
        "      x = [token_processing_fn(token) for token in x]\n",
        "      processed_review += x\n",
        "    tokenized_sentence_sentiment_list.append([review, processed_review, sentiment])\n",
        "  df = pd.DataFrame(tokenized_sentence_sentiment_list, columns=['original review','processed review','sentiment'])\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "nW59XNg_mJvQ"
      },
      "execution_count": 467,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReviewDataset(torch.utils.data.Dataset):\n",
        "  \"\"\"\n",
        "  Creates a dataframe with [review, sentiment_score]\n",
        "  Builds a vocabulary to map sentences into vectors\n",
        "  Whenever __getitem__ is called it returns (vectorized_sent, sentiment)\n",
        "  \"\"\"\n",
        "  def __init__(self, path_to_review_folder):\n",
        "        string_of_reviews = create_concat_text_content_string(path_to_review_folder)\n",
        "        self.review_df = process_string_of_reviews_into_df(string_of_reviews)\n",
        "        set_of_all_tokens = set([token for review in self.review_df['processed review'] for token in review])\n",
        "        self.vocab = build_vocab_from_iterator([set_of_all_tokens], max_tokens=len(set_of_all_tokens), specials=[\"<unk>\"])\n",
        "        print(f\"{len(self.vocab)=}\")\n",
        "        self.vocab.set_default_index(self.vocab[\"<unk>\"])\n",
        "        \n",
        "  def __len__(self):\n",
        "    return len(self.review_df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    review = self.review_df.iloc[idx]\n",
        "    return self.vocab(list(review[\"processed review\"])), review['sentiment']\n"
      ],
      "metadata": {
        "id": "d3Vjvkb4kDDf"
      },
      "execution_count": 468,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_REVIEWS = \"/content/drive/MyDrive/Colab Notebooks/product_reviews\"\n",
        "dataset = ReviewDataset(PATH_TO_REVIEWS)\n",
        "\"Example of a data sample\", dataset[1]"
      ],
      "metadata": {
        "id": "gqPx_Zhjn_Ws",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66fecebd-a702-4671-bd67-dcb03c0466e2"
      },
      "execution_count": 469,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After appending Nokia_6600.txt corpus sized increased to 0 -> 56093\n",
            "After appending norton.txt corpus sized increased to 56093 -> 95013\n",
            "After appending Linksys_Router.txt corpus sized increased to 95013 -> 151947\n",
            "After appending MicroMP3.txt corpus sized increased to 151947 -> 259727\n",
            "After appending Diaper_Champ.txt corpus sized increased to 259727 -> 294831\n",
            "After appending Hitachi_router.txt corpus sized increased to 294831 -> 325078\n",
            "After appending Canon_S100.txt corpus sized increased to 325078 -> 353887\n",
            "After appending Canon_PowerShot_SD500.txt corpus sized increased to 353887 -> 378520\n",
            "average_len used for ipod.txt = 10\n",
            "After appending ipod corpus sized increased to 378520 -> 436758\n",
            "len(self.vocab)=6006\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Example of a data sample',\n",
              " ([2722,\n",
              "   31,\n",
              "   2524,\n",
              "   5345,\n",
              "   862,\n",
              "   2309,\n",
              "   3541,\n",
              "   173,\n",
              "   3450,\n",
              "   3654,\n",
              "   619,\n",
              "   2722,\n",
              "   5494,\n",
              "   3207,\n",
              "   2939,\n",
              "   68,\n",
              "   5316,\n",
              "   3728,\n",
              "   1766,\n",
              "   4923,\n",
              "   2182,\n",
              "   2520,\n",
              "   871,\n",
              "   852,\n",
              "   3115,\n",
              "   68,\n",
              "   3399,\n",
              "   2426,\n",
              "   4460,\n",
              "   427,\n",
              "   1186,\n",
              "   2060,\n",
              "   299,\n",
              "   2696,\n",
              "   3755,\n",
              "   4923,\n",
              "   619,\n",
              "   2722,\n",
              "   1810,\n",
              "   3511,\n",
              "   4307,\n",
              "   5251,\n",
              "   3718,\n",
              "   2939,\n",
              "   5314,\n",
              "   3487,\n",
              "   68,\n",
              "   5856,\n",
              "   2928,\n",
              "   5345,\n",
              "   3950,\n",
              "   4923,\n",
              "   2491,\n",
              "   420,\n",
              "   4834,\n",
              "   68,\n",
              "   2939,\n",
              "   2520,\n",
              "   2939,\n",
              "   576,\n",
              "   68,\n",
              "   5345,\n",
              "   2928,\n",
              "   5316,\n",
              "   2250,\n",
              "   3950,\n",
              "   2722,\n",
              "   3027,\n",
              "   3696,\n",
              "   5314,\n",
              "   2520,\n",
              "   5316,\n",
              "   3055,\n",
              "   1011,\n",
              "   4653,\n",
              "   48,\n",
              "   2835,\n",
              "   48,\n",
              "   948,\n",
              "   48,\n",
              "   5649,\n",
              "   1399,\n",
              "   619,\n",
              "   427,\n",
              "   4990,\n",
              "   68,\n",
              "   2744,\n",
              "   5981,\n",
              "   695,\n",
              "   3190,\n",
              "   2309,\n",
              "   617,\n",
              "   5627,\n",
              "   48,\n",
              "   5345,\n",
              "   2928,\n",
              "   1642,\n",
              "   5316,\n",
              "   3950,\n",
              "   2309,\n",
              "   5981,\n",
              "   68],\n",
              "  8))"
            ]
          },
          "metadata": {},
          "execution_count": 469
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Implementation of the Classifier"
      ],
      "metadata": {
        "id": "zJP-2I1psElk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, num_class, drop_out=0.2):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.RNN(embed_dim, hidden_dim, dropout=drop_out,\n",
        "                          num_layers=num_layers, bidirectional=True)\n",
        "        # 2x because bidirectional RNN concats its outputs\n",
        "        self.fc = nn.Linear(2*hidden_dim, num_class) \n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        embedded = self.dropout(embedded)\n",
        "        y, hidden = self.rnn(embedded)\n",
        "        y = self.dropout(y)\n",
        "        y = self.fc(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "buZXAxPJ_TG-"
      },
      "execution_count": 483,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "  \"\"\"\n",
        "  This function allows for different inputs of different length to used in a single batch\n",
        "  \n",
        "  \"\"\"\n",
        "  label_list, text_list, offsets = [], [], [0]\n",
        "  for (_text, _label) in batch:\n",
        "        label_list.append(_label)\n",
        "        processed_text = torch.tensor(_text, dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        offsets.append(processed_text.size(0))\n",
        "  label_list = torch.tensor(label_list, dtype=torch.float32).reshape(-1, 1)\n",
        "  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "  text_list = torch.cat(text_list)\n",
        "  return label_list.to(DEVICE), text_list.to(DEVICE), offsets.to(DEVICE)"
      ],
      "metadata": {
        "id": "eEU39wmtmAfi"
      },
      "execution_count": 484,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Experiments to Evaluate the Classifier"
      ],
      "metadata": {
        "id": "e84CHqBcpLgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define logic required for training"
      ],
      "metadata": {
        "id": "bh4eyCW6YGpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DEFINE HYPERPARAMS\n",
        "EPOCHS = 10 \n",
        "LR = 1e-2 \n",
        "K_FOLDS = 5\n",
        "THRESHOLD = 0 # threshold used to classify a review as positive/negative\n",
        "BATCH_SIZE = 1\n",
        "TRAIN_PERCENTAGE = 0.8\n",
        "EMSIZE = 100 # embedding size used by the model\n",
        "HIDDEN_SIZE = 32 # size of the hidden layer\n",
        "NUM_LAYERS = 1\n",
        "DROP_OUT = 0.4\n",
        "vocab_size = len(dataset.vocab)\n",
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "DEVICE"
      ],
      "metadata": {
        "id": "Ye6aUoH4xzUd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7bfaf53-cc96-46ea-8772-b0b5f59216e1"
      },
      "execution_count": 485,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 485
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = TextClassificationModel(vocab_size, EMSIZE,\n",
        "                                HIDDEN_SIZE, NUM_LAYERS, num_class=1,\n",
        "                                drop_out=DROP_OUT).to(DEVICE)\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "id": "ElLRLbM9-OyY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaab5844-c170-4673-94db-76914f92e226"
      },
      "execution_count": 486,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 609,241 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training loop\n",
        "def train(dataloader):\n",
        "    correct_preds, total_preds, total_loss = 0, 0, 0\n",
        "    model.train()\n",
        "    for idx, (label, text, offsets) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        predicted_label = model(text, offsets)\n",
        "        loss = criterion(predicted_label, label)\n",
        "        loss.backward()\n",
        "        # prevent gradient explosion\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        # Threshold the predictions\n",
        "        correct_preds += ((predicted_label>THRESHOLD).float() == (label>THRESHOLD).float()).sum().item()\n",
        "        total_preds += label.size(0)\n",
        "    accuracy = correct_preds/total_preds\n",
        "    average_loss = total_loss/total_preds\n",
        "    return accuracy, average_loss"
      ],
      "metadata": {
        "id": "5MD1LXg_uHR7"
      },
      "execution_count": 487,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the validation loop\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    correct_preds, total_preds, total_loss = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predicted_label = model(text, offsets)\n",
        "            loss = criterion(predicted_label, label)\n",
        "            total_loss += loss.item()\n",
        "            correct_preds += ((predicted_label>THRESHOLD).float() == (label>THRESHOLD).float()).sum().item()\n",
        "            total_preds += label.size(0)\n",
        "    accuracy = correct_preds/total_preds\n",
        "    average_loss = total_loss/total_preds\n",
        "    return accuracy, average_loss"
      ],
      "metadata": {
        "id": "3x-4vknrztOT"
      },
      "execution_count": 488,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start training"
      ],
      "metadata": {
        "id": "K43v3RXnYLIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset\n",
        "train_val_size = int(TRAIN_PERCENTAGE*(len(dataset)))\n",
        "test_size = len(dataset) - train_val_size\n",
        "train_val_set, test_set = torch.utils.data.random_split(dataset, [train_val_size, test_size])\n",
        "kfold = KFold(n_splits=K_FOLDS)"
      ],
      "metadata": {
        "id": "WC8auOA1yIAz"
      },
      "execution_count": 489,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "_YUVA8-8y-Yr"
      },
      "execution_count": 490,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fold_val_accr = []\n",
        "fold_train_accr = []\n",
        "fold_test_accr = []\n",
        "# Start training\n",
        "for fold, (train_ids, valid_ids) in enumerate(kfold.split(train_val_set)):\n",
        "  print(f\"\\n\\n!!========== FOLD #{fold} ============!!\\n\")\n",
        "  # Create the dataloaders\n",
        "  train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "  valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)\n",
        "  train_dataloader = torch.utils.data.DataLoader(train_val_set, batch_size=BATCH_SIZE, collate_fn=collate_batch, sampler=train_subsampler)\n",
        "  val_dataloader = torch.utils.data.DataLoader(train_val_set, batch_size=BATCH_SIZE, collate_fn=collate_batch, sampler=valid_subsampler)\n",
        "  # Instantiate the model\n",
        "  model = TextClassificationModel(vocab_size, EMSIZE,\n",
        "                                  HIDDEN_SIZE, NUM_LAYERS, num_class=1,\n",
        "                                  drop_out=DROP_OUT).to(DEVICE)\n",
        "  # Instantiate optimization objects\n",
        "  criterion = torch.nn.L1Loss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "  # Keep track of losses throughout the epoch\n",
        "  higest_val_accuracy = 0\n",
        "  train_losses = []\n",
        "  train_accrs = []\n",
        "  val_losses = []\n",
        "  val_accrs = []\n",
        "  for epoch in range(1, EPOCHS + 1):\n",
        "      train_acc, train_loss = train(train_dataloader)\n",
        "      val_acc, val_loss = evaluate(val_dataloader)\n",
        "      # if has accuracy is lower than in prev epoch - reduce the learning rate\n",
        "      if higest_val_accuracy > val_acc:\n",
        "        scheduler.step()\n",
        "      highest_val_accuracy = max(higest_val_accuracy, val_acc)\n",
        "      print('-' * 80)\n",
        "      print(f'end of {epoch=} | {train_acc=:.4f} | {train_loss=:.6f} | {val_acc=:.4f} ')\n",
        "      print('-' * 80)\n",
        "      train_losses.append(train_loss)\n",
        "      train_accrs.append(train_acc)\n",
        "      val_accrs.append(val_acc)\n",
        "      val_losses.append(val_loss)\n",
        "  # append the last and (hopefully) highest accuracies \n",
        "  fold_val_accr.append(val_accrs[-1])\n",
        "  fold_train_accr.append(train_accrs[-1])\n",
        "  test_acc, test_loss = evaluate(test_dataloader)\n",
        "  print(\"End of Fold | test_acc\", test_acc)\n",
        "  fold_test_accr.append(test_acc)"
      ],
      "metadata": {
        "id": "2gdzO93omPfV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493,
          "referenced_widgets": [
            "46b5745ec94d420898995a4f04d92be0",
            "65ababf4c3df4746934921b8dca6242a",
            "bde8330d70344178a7113107e5dfa167",
            "3a2df7bc36fd428d8a82367c7cde6b56",
            "7656bcc08e6b4175bfc19a51ebb1ce49",
            "d5e97d0fdcf6464ca30996af52a9f24b",
            "5738c65c6fb640fd92a5d83ebdeca4e0",
            "4475a418e044433e8a5abc15070667d8",
            "19201ac17d194075820271e57b09186a",
            "13e31c97ba54465f96d0d15924d0e951",
            "a810405ccd7a4834b6339e26b1185bd2"
          ]
        },
        "outputId": "1a5f1b71-c867-4a43-90ea-2c28738e11e9"
      },
      "execution_count": 492,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "!!========== FOLD #0 ============!!\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/207 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46b5745ec94d420898995a4f04d92be0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-492-89ba65fb247d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mval_accrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m       \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0;31m# if has accuracy is lower than in prev epoch - reduce the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-487-9786a5935001>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mpredicted_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-483-08972d30e81c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text, offsets)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examine the results"
      ],
      "metadata": {
        "id": "Ccd6HW6UYOu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I look at how the training went throughout the last epoch\n",
        "This allows me to see whether the model overfits"
      ],
      "metadata": {
        "id": "ol3JhmDGdobY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plotStatThroughoutEpochs(label, train_stat, validation_stat):\n",
        "  plt.plot(train_stat, label=f'Training {label}')\n",
        "  plt.plot(validation_stat, label=f'Validation {label}')\n",
        "  plt.xticks(range(len(train_stat)))\n",
        "  plt.title(f'{label} over time')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel(f'{label}')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "xu6Vx8QIiQnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotStatThroughoutEpochs(\"Loss\", train_losses, val_losses)"
      ],
      "metadata": {
        "id": "H3Egu2vG1Vk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotStatThroughoutEpochs(\"Accuracy\", train_accrs, val_accrs)"
      ],
      "metadata": {
        "id": "UBSzXp-w07dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Examine results from different folds\n",
        "Ideally we want to see low variance here"
      ],
      "metadata": {
        "id": "axhqu0lDfZ9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title(\"Final epoch validation accuracies for each fold\")\n",
        "plt.xlabel(\"k-Folds\")\n",
        "plt.ylabel(\"Accuracy in the last epoch\")\n",
        "plt.bar(range(K_FOLDS), fold_val_accr)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iCKhyNN4fgYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <mark>Test against custom input</mark>"
      ],
      "metadata": {
        "id": "Qmijf79NfwBk"
      }
    }
  ]
}